{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33movsvc\u001b[0m (\u001b[33movsvc-tu-wien\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/viktoriiaovsianik/.netrc\n"
     ]
    }
   ],
   "source": [
    "#Imports \n",
    "import sys\n",
    "sys.path.append('/Users/viktoriiaovsianik/Documents/Uni/04_WS2024/06_ADL/Code/ADL-WS-2024')\n",
    "from scripts.train_cnn import train_model\n",
    "from scripts.viz import display_images\n",
    "\n",
    "from datasets.AIArtBench import AIArtbench\n",
    "from datasets.dataset import Subset\n",
    "from datasets.preprocessing import CustomDatasetPreprocessor\n",
    "from torch.utils.data import DataLoader\n",
    "from models.cnn import CNN_Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset path and preprocess the data\n",
    "dataset_path = \"/Users/viktoriiaovsianik/Documents/Uni/04_WS2024/06_ADL/Code/ADL-WS-2024/data\"\n",
    "preprocessor = CustomDatasetPreprocessor(dataset_path)\n",
    "preprocessor.preprocess(fraction=0.2)\n",
    "\n",
    "# Get dataset splits\n",
    "train_data, validation_data, test_data = preprocessor.get_splits()\n",
    "\n",
    "# Prepare the training dataset and DataLoader\n",
    "train_dataset = AIArtbench(dataframe=train_data, subset=Subset.TRAINING, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AIArtbench_opt' object has no attribute 'plot_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Plot images\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_images\u001b[49m(label_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mplot_images(label_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m'\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract images\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get unique class names\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#classes = train_data['label'].unique()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Display a few images from the training data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#display_images(data_loader=train_data_loader, classes=classes, num_images=8, save_path=\"train_images_grid.png\")\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AIArtbench_opt' object has no attribute 'plot_images'"
     ]
    }
   ],
   "source": [
    "#Plot images\n",
    "train_dataset.plot_images(label_type='human', k=5)\n",
    "train_dataset.plot_images(label_type='AI', k=5)\n",
    "\n",
    "# Extract images\n",
    "#train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# Get unique class names\n",
    "#classes = train_data['label'].unique()\n",
    "# Display a few images from the training data\n",
    "#display_images(data_loader=train_data_loader, classes=classes, num_images=8, save_path=\"train_images_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) is not available. Training on CPU.\n",
      "Preprocessing dataset...\n",
      "Train dataset length: 1394\n",
      "Validation dataset length: 156\n",
      "Preparing data transforms...\n",
      "Loading training and validation data...\n",
      "Check loaded data: 1394\n",
      "Setting up the model and optimizer...\n",
      "Model: CNN_Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/viktoriiaovsianik/Documents/Uni/04_WS2024/06_ADL/Code/ADL-WS-2024/notebooks/wandb/run-20241216_165650-0icyofuy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ovsvc-tu-wien/ADL/runs/0icyofuy' target=\"_blank\">major-spaceship-46</a></strong> to <a href='https://wandb.ai/ovsvc-tu-wien/ADL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ovsvc-tu-wien/ADL' target=\"_blank\">https://wandb.ai/ovsvc-tu-wien/ADL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ovsvc-tu-wien/ADL/runs/0icyofuy' target=\"_blank\">https://wandb.ai/ovsvc-tu-wien/ADL/runs/0icyofuy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size: 64\n",
      "Epoch 0/5:\n",
      "--- Training epoch 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▍         | 1/22 [00:00<00:06,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.0111677646636963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 11/22 [00:02<00:02,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 3.02372670173645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 22/22 [00:04<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20, Loss: 3.0068624019622803\n",
      "Epoch 0 Training Loss: 3.0101572769444163\n",
      "Training Metrics: Overall Accuracy: 0.0265\n",
      " Per-Class Accuracy:\n",
      "Accuracy for class AI_ukiyo-e: 0.00\n",
      "Accuracy for class AI_impressionism: 0.00\n",
      "Accuracy for class human_baroque: 0.00\n",
      "Accuracy for class AI_surrealism: 0.00\n",
      "Accuracy for class AI_romanticism: 0.00\n",
      "Accuracy for class AI_post_impressionism: 0.00\n",
      "Accuracy for class human_realism: 0.00\n",
      "Accuracy for class human_expressionism: 0.00\n",
      "Accuracy for class human_romanticism: 0.00\n",
      "Accuracy for class human_impressionism: 0.01\n",
      "Accuracy for class AI_art_nouveau: 0.00\n",
      "Accuracy for class AI_realism: 0.00\n",
      "Accuracy for class AI_renaissance: 0.00\n",
      "Accuracy for class AI_expressionism: 0.00\n",
      "Accuracy for class human_art_nouveau: 0.00\n",
      "Accuracy for class human_ukiyo_e: 0.94\n",
      "Accuracy for class human_post_impressionism: 0.05\n",
      "Accuracy for class human_surrealism: 0.00\n",
      "Accuracy for class AI_baroque: 0.00\n",
      "Accuracy for class human_renaissance: 0.00\n",
      "--- Validating epoch 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate:  67%|██████▋   | 2/3 [00:00<00:00,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Validation Loss: 3.0177180767059326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 3/3 [00:00<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss: 3.017298380533854\n",
      "Validation Metrics: Overall Accuracy: 0.0321\n",
      " Per-Class Accuracy:\n",
      "Accuracy for class human_art_nouveau: 0.00\n",
      "Accuracy for class AI_ukiyo-e: 0.00\n",
      "Accuracy for class AI_impressionism: 0.00\n",
      "Accuracy for class AI_surrealism: 0.00\n",
      "Accuracy for class AI_baroque: 0.00\n",
      "Accuracy for class AI_expressionism: 0.00\n",
      "Accuracy for class AI_realism: 0.00\n",
      "Accuracy for class AI_renaissance: 0.00\n",
      "Accuracy for class human_surrealism: 0.00\n",
      "Accuracy for class AI_romanticism: 0.00\n",
      "Accuracy for class human_baroque: 0.00\n",
      "Accuracy for class human_romanticism: 0.00\n",
      "Accuracy for class AI_art_nouveau: 0.00\n",
      "Accuracy for class human_renaissance: 0.00\n",
      "Accuracy for class human_impressionism: 0.00\n",
      "Accuracy for class human_ukiyo_e: 1.00\n",
      "Accuracy for class human_expressionism: 0.00\n",
      "Accuracy for class human_realism: 0.00\n",
      "Accuracy for class AI_post_impressionism: 0.00\n",
      "Accuracy for class human_post_impressionism: 0.00\n",
      "#### Best accuracy 0.03205128205128205 at epoch 0\n",
      "#### Saving model to saved_models\n",
      "Model saved to saved_models/model_best.pth\n",
      "Epoch 1/5:\n",
      "--- Training epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▍         | 1/22 [00:00<00:05,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.0165765285491943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 11/22 [00:02<00:02,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 3.006169319152832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 22/22 [00:04<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20, Loss: 3.0061264038085938\n",
      "Epoch 1 Training Loss: 3.0079301388739172\n",
      "Training Metrics: Overall Accuracy: 0.0588\n",
      " Per-Class Accuracy:\n",
      "Accuracy for class AI_ukiyo-e: 0.00\n",
      "Accuracy for class AI_impressionism: 0.00\n",
      "Accuracy for class human_baroque: 0.00\n",
      "Accuracy for class AI_surrealism: 0.00\n",
      "Accuracy for class AI_romanticism: 0.00\n",
      "Accuracy for class AI_post_impressionism: 0.00\n",
      "Accuracy for class human_realism: 0.00\n",
      "Accuracy for class human_expressionism: 0.00\n",
      "Accuracy for class human_romanticism: 0.00\n",
      "Accuracy for class human_impressionism: 0.59\n",
      "Accuracy for class AI_art_nouveau: 0.00\n",
      "Accuracy for class AI_realism: 0.00\n",
      "Accuracy for class AI_renaissance: 0.00\n",
      "Accuracy for class AI_expressionism: 0.00\n",
      "Accuracy for class human_art_nouveau: 0.00\n",
      "Accuracy for class human_ukiyo_e: 0.47\n",
      "Accuracy for class human_post_impressionism: 0.00\n",
      "Accuracy for class human_surrealism: 0.00\n",
      "Accuracy for class AI_baroque: 0.00\n",
      "Accuracy for class human_renaissance: 0.00\n",
      "Epoch 2/5:\n",
      "--- Training epoch 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▍         | 1/22 [00:00<00:04,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.0082643032073975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 11/22 [00:02<00:02,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 2.996680736541748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  95%|█████████▌| 21/22 [00:04<00:00,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20, Loss: 3.0058071613311768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 22/22 [00:05<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 3.005722569256294\n",
      "Training Metrics: Overall Accuracy: 0.0789\n",
      " Per-Class Accuracy:\n",
      "Accuracy for class AI_ukiyo-e: 0.00\n",
      "Accuracy for class AI_impressionism: 0.00\n",
      "Accuracy for class human_baroque: 0.00\n",
      "Accuracy for class AI_surrealism: 0.00\n",
      "Accuracy for class AI_romanticism: 0.00\n",
      "Accuracy for class AI_post_impressionism: 0.00\n",
      "Accuracy for class human_realism: 0.00\n",
      "Accuracy for class human_expressionism: 0.00\n",
      "Accuracy for class human_romanticism: 0.00\n",
      "Accuracy for class human_impressionism: 1.00\n",
      "Accuracy for class AI_art_nouveau: 0.00\n",
      "Accuracy for class AI_realism: 0.00\n",
      "Accuracy for class AI_renaissance: 0.00\n",
      "Accuracy for class AI_expressionism: 0.00\n",
      "Accuracy for class human_art_nouveau: 0.00\n",
      "Accuracy for class human_ukiyo_e: 0.00\n",
      "Accuracy for class human_post_impressionism: 0.00\n",
      "Accuracy for class human_surrealism: 0.00\n",
      "Accuracy for class AI_baroque: 0.00\n",
      "Accuracy for class human_renaissance: 0.00\n",
      "Epoch 3/5:\n",
      "--- Training epoch 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▍         | 1/22 [00:00<00:05,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 3.004335641860962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 11/22 [00:02<00:02,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 3.0014259815216064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 22/22 [00:05<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20, Loss: 2.9979939460754395\n",
      "Epoch 3 Training Loss: 3.0036597409241512\n",
      "Training Metrics: Overall Accuracy: 0.0789\n",
      " Per-Class Accuracy:\n",
      "Accuracy for class AI_ukiyo-e: 0.00\n",
      "Accuracy for class AI_impressionism: 0.00\n",
      "Accuracy for class human_baroque: 0.00\n",
      "Accuracy for class AI_surrealism: 0.00\n",
      "Accuracy for class AI_romanticism: 0.00\n",
      "Accuracy for class AI_post_impressionism: 0.00\n",
      "Accuracy for class human_realism: 0.00\n",
      "Accuracy for class human_expressionism: 0.00\n",
      "Accuracy for class human_romanticism: 0.00\n",
      "Accuracy for class human_impressionism: 1.00\n",
      "Accuracy for class AI_art_nouveau: 0.00\n",
      "Accuracy for class AI_realism: 0.00\n",
      "Accuracy for class AI_renaissance: 0.00\n",
      "Accuracy for class AI_expressionism: 0.00\n",
      "Accuracy for class human_art_nouveau: 0.00\n",
      "Accuracy for class human_ukiyo_e: 0.00\n",
      "Accuracy for class human_post_impressionism: 0.00\n",
      "Accuracy for class human_surrealism: 0.00\n",
      "Accuracy for class AI_baroque: 0.00\n",
      "Accuracy for class human_renaissance: 0.00\n",
      "--- Validating epoch 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate:  33%|███▎      | 1/3 [00:00<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Validation Loss: 3.0112757682800293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 3/3 [00:00<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 3.011140908950414\n",
      "Validation Metrics: Overall Accuracy: 0.0513\n",
      " Per-Class Accuracy:\n",
      "Accuracy for class human_art_nouveau: 0.00\n",
      "Accuracy for class AI_ukiyo-e: 0.00\n",
      "Accuracy for class AI_impressionism: 0.00\n",
      "Accuracy for class AI_surrealism: 0.00\n",
      "Accuracy for class AI_baroque: 0.00\n",
      "Accuracy for class AI_expressionism: 0.00\n",
      "Accuracy for class AI_realism: 0.00\n",
      "Accuracy for class AI_renaissance: 0.00\n",
      "Accuracy for class human_surrealism: 0.00\n",
      "Accuracy for class AI_romanticism: 1.00\n",
      "Accuracy for class human_baroque: 0.00\n",
      "Accuracy for class human_romanticism: 0.00\n",
      "Accuracy for class AI_art_nouveau: 0.00\n",
      "Accuracy for class human_renaissance: 0.00\n",
      "Accuracy for class human_impressionism: 0.00\n",
      "Accuracy for class human_ukiyo_e: 0.00\n",
      "Accuracy for class human_expressionism: 0.00\n",
      "Accuracy for class human_realism: 0.00\n",
      "Accuracy for class AI_post_impressionism: 0.00\n",
      "Accuracy for class human_post_impressionism: 0.00\n",
      "#### Best accuracy 0.05128205128205128 at epoch 3\n",
      "#### Saving model to saved_models\n",
      "Model saved to saved_models/model_best.pth\n",
      "Epoch 4/5:\n",
      "--- Training epoch 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▍         | 1/22 [00:00<00:05,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.997983694076538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 11/22 [00:02<00:02,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 3.009843111038208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 22/22 [00:05<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20, Loss: 2.9942219257354736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 22/22 [00:05<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 3.00195654085069\n",
      "Training Metrics: Overall Accuracy: 0.0789\n",
      " Per-Class Accuracy:\n",
      "Accuracy for class AI_ukiyo-e: 0.00\n",
      "Accuracy for class AI_impressionism: 0.00\n",
      "Accuracy for class human_baroque: 0.00\n",
      "Accuracy for class AI_surrealism: 0.00\n",
      "Accuracy for class AI_romanticism: 0.00\n",
      "Accuracy for class AI_post_impressionism: 0.00\n",
      "Accuracy for class human_realism: 0.00\n",
      "Accuracy for class human_expressionism: 0.00\n",
      "Accuracy for class human_romanticism: 0.00\n",
      "Accuracy for class human_impressionism: 1.00\n",
      "Accuracy for class AI_art_nouveau: 0.00\n",
      "Accuracy for class AI_realism: 0.00\n",
      "Accuracy for class AI_renaissance: 0.00\n",
      "Accuracy for class AI_expressionism: 0.00\n",
      "Accuracy for class human_art_nouveau: 0.00\n",
      "Accuracy for class human_ukiyo_e: 0.00\n",
      "Accuracy for class human_post_impressionism: 0.00\n",
      "Accuracy for class human_surrealism: 0.00\n",
      "Accuracy for class AI_baroque: 0.00\n",
      "Accuracy for class human_renaissance: 0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/acc</td><td>▁▅███</td></tr><tr><td>train/loss</td><td>█▆▄▂▁</td></tr><tr><td>val/acc</td><td>▁█</td></tr><tr><td>val/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/acc</td><td>0.07891</td></tr><tr><td>train/loss</td><td>3.00196</td></tr><tr><td>val/acc</td><td>0.05128</td></tr><tr><td>val/loss</td><td>3.01114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-spaceship-46</strong> at: <a href='https://wandb.ai/ovsvc-tu-wien/ADL/runs/0icyofuy' target=\"_blank\">https://wandb.ai/ovsvc-tu-wien/ADL/runs/0icyofuy</a><br/> View project at: <a href='https://wandb.ai/ovsvc-tu-wien/ADL' target=\"_blank\">https://wandb.ai/ovsvc-tu-wien/ADL</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_165650-0icyofuy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset_path\": dataset_path,  # Modify as needed\n",
    "    \"fraction\": 0.1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"scheduler_gamma\": 0.9,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 64,\n",
    "    \"val_frequency\": 3,\n",
    "    \"model_save_dir\": \"saved_models\",\n",
    "    \"debug_mode\": \"True\",\n",
    "    \"model\": model\n",
    "}\n",
    "\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) is not available. Training on CPU.\n",
      "Preprocessing dataset...\n",
      "Train dataset length: 53508\n",
      "Validation dataset length: 5949\n",
      "Preparing data transforms...\n",
      "Loading training and validation data...\n",
      "Check loaded data: 53508\n",
      "Setting up the model and optimizer...\n",
      "Model: Simple_CNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=20, bias=True)\n",
      ")\n",
      "Training with batch size: 64\n",
      "Epoch 0/5:\n",
      "--- Training epoch 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 1/837 [00:00<04:59,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.9966914653778076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   1%|▏         | 11/837 [00:03<05:10,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 3.0046849250793457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   2%|▏         | 13/837 [00:04<05:00,  2.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m      3\u001b[0m model_simple \u001b[38;5;241m=\u001b[39m Simple_CNN()\n\u001b[1;32m      5\u001b[0m config_simple \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_path,  \u001b[38;5;66;03m# Modify as needed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_simple\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_simple\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Uni/04_WS2024/06_ADL/Code/ADL-WS-2024/scripts/train_cnn.py:109\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    107\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    108\u001b[0m trainer \u001b[38;5;241m=\u001b[39m initialize_trainer(config, device)\n\u001b[0;32m--> 109\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), Path(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_save_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN_custom.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m trainer\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m~/Documents/Uni/04_WS2024/06_ADL/Code/ADL-WS-2024/trainers/trainer.py:211\u001b[0m, in \u001b[0;36mImgClassificationTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m     train_loss, train_acc, train_acc_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     wandb_log \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/acc\u001b[39m\u001b[38;5;124m'\u001b[39m: train_acc}\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Uni/04_WS2024/06_ADL/Code/ADL-WS-2024/trainers/trainer.py:111\u001b[0m, in \u001b[0;36mImgClassificationTrainer._train_epoch\u001b[0;34m(self, epoch_idx)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metric\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    108\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 111\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/Uni/04_WS2024/06_ADL/Code/ADL-WS-2024/datasets/AIArtBench_opt.py:58\u001b[0m, in \u001b[0;36mAIArtbench_opt.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Apply transformations if provided\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 58\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Convert the label to a tensor\u001b[39;00m\n\u001b[1;32m     61\u001b[0m y_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_idx, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/torchvision/transforms/functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    166\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 167\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(pic, mode_to_nptype\u001b[38;5;241m.\u001b[39mget(pic\u001b[38;5;241m.\u001b[39mmode, np\u001b[38;5;241m.\u001b[39muint8), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    170\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/PIL/Image.py:756\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 756\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.11/site-packages/PIL/Image.py:818\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    816\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     bytes_consumed, errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.simple_cnn import Simple_CNN\n",
    "\n",
    "model_simple = Simple_CNN()\n",
    "\n",
    "config_simple = {\n",
    "    \"dataset_path\": dataset_path,  # Modify as needed\n",
    "    \"fraction\": 0.7,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"scheduler_gamma\": 0.9,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 64,\n",
    "    \"val_frequency\": 3,\n",
    "    \"model_save_dir\": \"saved_models\",\n",
    "    \"debug_mode\": \"True\",\n",
    "    \"model\": model_simple\n",
    "}\n",
    "\n",
    "train_model(config_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
